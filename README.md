# DAT500
Data Intensive Systems

Today big data analysis has become a routine work for many organizations. Designing the data pipeline and making sense of huge amount data generated every day is a big challenge. This has led to development of platforms and languages which are suited for handling large amount of data and provide the best performance in a distributed system. Hadoop and Spark are two most common opens source engines/platform which provide this service. This technical paper presents a case study on the analysis of the New York City (NYC) taxi dataset using Hadoop and Spark, with a primary focus on optimizing Spark performance. The motivation of this study is to gain valuable insights into popular pickup and drop-off locations, peak hours, and other trends that can inform transportation policy decisions and enhance the overall taxi experience. Primary goal of this project was to show Spark optimization handling big data and demonstrate the efficiency that can be gained. Various optimization methods have been demonstrated in this project. 
Data skewness can also slow down the speed substantially. This is because of uneven distribution of data on different partitions. Repartitioning and using the salting technique resulted in reducing the execution time by 42%. 
Broadcast join with a smaller table was compared to the normal join operation. The broadcast join takes only half the time compared to the normal join. However, in this used case the join operation takes very small time and doesnâ€™t have much impact on the overall execution time.  
Caching and persist techniques have shown considerable impact on the overall execution time. When the intermediate results after cleaning and filtering the data was cached, it reduced the execution time by 25%. 
Optimization on the Spark config setting was done. These include changing the spark session config parameters namely the number of executors, number of cores per executor, number of partitions and memory allocation. Given the cluster configuration an optimum config setting for Spark was found which improved the execution time by 40%. 
Data storage and data read write operations can be substantially optimized by using parquet and delta format rather than CSV format. Reading the input from parquet format takes only 20% of the time compared to the CSV format. 
